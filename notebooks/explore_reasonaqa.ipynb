{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReasonAQA Dataset Exploration\n",
    "\n",
    "This notebook helps you explore and understand the ReasonAQA dataset structure, content, and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (3.10.8)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ikulkar1/miniconda3/envs/flux_finetuning/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ikulkar1/qwen_omni_finetune/audio_reasoning_interspeech/src/data/reasonaqa/reasonaqa\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Base path\n",
    "BASE_PATH = Path('/home/ikulkar1/qwen_omni_finetune/audio_reasoning_interspeech')\n",
    "DATA_PATH = BASE_PATH / 'src/data/reasonaqa/reasonaqa'\n",
    "\n",
    "print(f\"Data directory: {DATA_PATH}\")\n",
    "print(f\"Directory exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train: 968,071 samples\n",
      "Loaded val: 114,188 samples\n",
      "Loaded test: 161,695 samples\n",
      "\n",
      "Total samples: 1,243,954\n"
     ]
    }
   ],
   "source": [
    "def load_split(split_name):\n",
    "    \"\"\"Load a dataset split.\"\"\"\n",
    "    file_path = DATA_PATH / f\"{split_name}.json\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {split_name}: {len(data):,} samples\")\n",
    "    return data\n",
    "\n",
    "# Load all splits\n",
    "train_data = load_split('train')\n",
    "val_data = load_split('val')\n",
    "test_data = load_split('test')\n",
    "\n",
    "print(f\"\\nTotal samples: {len(train_data) + len(val_data) + len(test_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Structure Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample structure (first entry from test set):\n",
      "{\n",
      "  \"taskname\": \"audiocaps\",\n",
      "  \"filepath1\": \"AudioCapsLarger/test/Y7fmOlUlwoNg.wav\",\n",
      "  \"filepath2\": \"AudioCapsLarger/test/YZYWCwfCkBp4.wav\",\n",
      "  \"caption1\": \"Constant rattling noise and sharp vibrations\",\n",
      "  \"caption2\": \"Sawing wood with music playing in the distance\",\n",
      "  \"input\": \"explain the difference in few words\",\n",
      "  \"answer\": \"Audio 1 features a constant, high-frequency rattling noise with sharp vibrations, while Audio 2 combines a mid-frequency sawing sound with a distant, low-frequency music accompaniment.\",\n",
      "  \"subtype\": \"ACD-1.json\"\n",
      "}\n",
      "\n",
      "Keys in each sample:\n",
      "['taskname', 'filepath1', 'filepath2', 'caption1', 'caption2', 'input', 'answer', 'subtype']\n"
     ]
    }
   ],
   "source": [
    "# Show first sample structure\n",
    "print(\"Sample structure (first entry from test set):\")\n",
    "print(json.dumps(test_data[0], indent=2))\n",
    "\n",
    "print(\"\\nKeys in each sample:\")\n",
    "print(list(test_data[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET STATISTICS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "TRAIN SPLIT:\n",
      "  Total samples: 968,071\n",
      "  Dual audio samples: 203,565\n",
      "  Single audio samples: 764,506\n",
      "\n",
      "  Task breakdown:\n",
      "    audiocaps: 675,303\n",
      "    clothov21: 278,680\n",
      "    clotho_aqa_train: 14,088\n",
      "\n",
      "VAL SPLIT:\n",
      "  Total samples: 114,188\n",
      "  Dual audio samples: 23,043\n",
      "  Single audio samples: 91,145\n",
      "\n",
      "  Task breakdown:\n",
      "    clothov21: 75,871\n",
      "    audiocaps: 34,189\n",
      "    clotho_aqa_val: 4,128\n",
      "\n",
      "TEST SPLIT:\n",
      "  Total samples: 161,695\n",
      "  Dual audio samples: 29,715\n",
      "  Single audio samples: 131,980\n",
      "\n",
      "  Task breakdown:\n",
      "    audiocaps: 80,102\n",
      "    clothov21: 75,917\n",
      "    clotho_aqa_test: 5,676\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_stats(data, split_name):\n",
    "    \"\"\"Get comprehensive statistics about a dataset split.\"\"\"\n",
    "    stats = {\n",
    "        'split': split_name,\n",
    "        'total_samples': len(data),\n",
    "        'tasknames': Counter([item['taskname'] for item in data]),\n",
    "        'subtypes': Counter([item['subtype'] for item in data]),\n",
    "        'has_audio1': sum(1 for item in data if item['filepath1']),\n",
    "        'has_audio2': sum(1 for item in data if item['filepath2']),\n",
    "        'dual_audio': sum(1 for item in data if item['filepath1'] and item['filepath2']),\n",
    "        'single_audio': sum(1 for item in data if item['filepath1'] and not item['filepath2']),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Get statistics for all splits\n",
    "train_stats = get_dataset_stats(train_data, 'train')\n",
    "val_stats = get_dataset_stats(val_data, 'val')\n",
    "test_stats = get_dataset_stats(test_data, 'test')\n",
    "\n",
    "# Display summary\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET STATISTICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for stats in [train_stats, val_stats, test_stats]:\n",
    "    print(f\"\\n{stats['split'].upper()} SPLIT:\")\n",
    "    print(f\"  Total samples: {stats['total_samples']:,}\")\n",
    "    print(f\"  Dual audio samples: {stats['dual_audio']:,}\")\n",
    "    print(f\"  Single audio samples: {stats['single_audio']:,}\")\n",
    "    print(f\"\\n  Task breakdown:\")\n",
    "    for task, count in stats['tasknames'].most_common():\n",
    "        print(f\"    {task}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Task Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of task distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (data, title) in zip(axes, \n",
    "    [(train_data, 'Train'), (val_data, 'Val'), (test_data, 'Test')]):\n",
    "    \n",
    "    tasknames = Counter([item['taskname'] for item in data])\n",
    "    \n",
    "    ax.bar(tasknames.keys(), tasknames.values())\n",
    "    ax.set_title(f'{title} Split - Task Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Task Name')\n",
    "    ax.set_ylabel('Number of Samples')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (task, count) in enumerate(tasknames.items()):\n",
    "        ax.text(i, count, f'{count:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore Different Subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 15 subtypes\n",
    "print(\"TOP 15 SUBTYPES IN TEST SET:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "subtypes = Counter([item['subtype'] for item in test_data])\n",
    "for i, (subtype, count) in enumerate(subtypes.most_common(15), 1):\n",
    "    print(f\"{i:2d}. {subtype:<40} {count:>6,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example Samples from Different Task Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from different subtypes\n",
    "subtypes_to_show = ['ACD-1.json', 'ACD-2.json', 'ACE.json', 'AudioCaps-MCQ.json', 'AudioCaps-Detail.json']\n",
    "\n",
    "print(\"EXAMPLE SAMPLES FROM DIFFERENT SUBTYPES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for subtype in subtypes_to_show:\n",
    "    # Find first sample with this subtype\n",
    "    sample = next((item for item in test_data if item['subtype'] == subtype), None)\n",
    "    if sample:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SUBTYPE: {subtype}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(json.dumps(sample, indent=2))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Audio File Path Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze audio file paths\n",
    "def analyze_audio_paths(data):\n",
    "    \"\"\"Analyze audio file paths to understand directory structure.\"\"\"\n",
    "    paths1 = [item['filepath1'] for item in data if item['filepath1']]\n",
    "    paths2 = [item['filepath2'] for item in data if item['filepath2']]\n",
    "    \n",
    "    # Extract directories\n",
    "    dirs1 = [Path(p).parts[0] if p else None for p in paths1]\n",
    "    dirs2 = [Path(p).parts[0] if p else None for p in paths2]\n",
    "    \n",
    "    return Counter(dirs1), Counter(dirs2)\n",
    "\n",
    "dirs1, dirs2 = analyze_audio_paths(test_data)\n",
    "\n",
    "print(\"AUDIO FILE DIRECTORIES (filepath1):\")\n",
    "for dir_name, count in dirs1.most_common():\n",
    "    print(f\"  {dir_name}: {count:,} files\")\n",
    "\n",
    "print(\"\\nAUDIO FILE DIRECTORIES (filepath2):\")\n",
    "for dir_name, count in dirs2.most_common():\n",
    "    print(f\"  {dir_name}: {count:,} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Input and Answer Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze question and answer lengths\n",
    "input_lengths = [len(item['input']) for item in test_data]\n",
    "answer_lengths = [len(item['answer']) for item in test_data]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Input lengths\n",
    "axes[0].hist(input_lengths, bins=50, edgecolor='black')\n",
    "axes[0].set_title('Question/Input Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(sum(input_lengths)/len(input_lengths), color='r', linestyle='--', label=f'Mean: {sum(input_lengths)/len(input_lengths):.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Answer lengths\n",
    "axes[1].hist(answer_lengths, bins=50, edgecolor='black')\n",
    "axes[1].set_title('Answer Length Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Character Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(sum(answer_lengths)/len(answer_lengths), color='r', linestyle='--', label=f'Mean: {sum(answer_lengths)/len(answer_lengths):.1f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Input length - Min: {min(input_lengths)}, Max: {max(input_lengths)}, Avg: {sum(input_lengths)/len(input_lengths):.1f}\")\n",
    "print(f\"Answer length - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}, Avg: {sum(answer_lengths)/len(answer_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Single vs Dual Audio Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs dual audio tasks\n",
    "single_audio = [item for item in test_data if item['filepath1'] and not item['filepath2']]\n",
    "dual_audio = [item for item in test_data if item['filepath1'] and item['filepath2']]\n",
    "\n",
    "print(f\"Single audio tasks: {len(single_audio):,} ({len(single_audio)/len(test_data)*100:.1f}%)\")\n",
    "print(f\"Dual audio tasks: {len(dual_audio):,} ({len(dual_audio)/len(test_data)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nSubtypes for SINGLE audio tasks:\")\n",
    "single_subtypes = Counter([item['subtype'] for item in single_audio])\n",
    "for subtype, count in single_subtypes.most_common(10):\n",
    "    print(f\"  {subtype}: {count:,}\")\n",
    "\n",
    "print(\"\\nSubtypes for DUAL audio tasks:\")\n",
    "dual_subtypes = Counter([item['subtype'] for item in dual_audio])\n",
    "for subtype, count in dual_subtypes.most_common(10):\n",
    "    print(f\"  {subtype}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create DataFrame for Easy Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier exploration\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "# Add helper columns\n",
    "df_test['has_dual_audio'] = df_test['filepath2'].apply(lambda x: bool(x))\n",
    "df_test['input_length'] = df_test['input'].apply(len)\n",
    "df_test['answer_length'] = df_test['answer'].apply(len)\n",
    "\n",
    "print(\"DataFrame shape:\", df_test.shape)\n",
    "print(\"\\nColumn names:\")\n",
    "print(df_test.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Filter and Search Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find all MCQ (Multiple Choice Question) tasks\n",
    "mcq_samples = df_test[df_test['subtype'].str.contains('MCQ', na=False)]\n",
    "print(f\"Found {len(mcq_samples)} MCQ samples\")\n",
    "print(\"\\nExample MCQ sample:\")\n",
    "print(json.dumps(mcq_samples.iloc[0].to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find samples with short answers (potential multiple choice)\n",
    "short_answers = df_test[df_test['answer_length'] < 50]\n",
    "print(f\"Found {len(short_answers)} samples with short answers (<50 chars)\")\n",
    "print(\"\\nSample short answers:\")\n",
    "for i, row in short_answers.head(5).iterrows():\n",
    "    print(f\"  Input: {row['input'][:60]}...\")\n",
    "    print(f\"  Answer: {row['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Sample Subset for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a small subset for testing\n",
    "subset_size = 100\n",
    "test_subset = test_data[:subset_size]\n",
    "\n",
    "output_path = BASE_PATH / 'notebooks/reasonaqa_test_subset.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(test_subset, f, indent=2)\n",
    "\n",
    "print(f\"Exported {subset_size} samples to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Custom Exploration\n",
    "\n",
    "Use the cells below for your own custom exploration of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom exploration code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
